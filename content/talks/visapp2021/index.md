---
title: 'Building the Next Generation of Virtual Personal Assistants with First Person (Egocentric) Vision: From Visual Intelligence to AI and Future Predictions'
type: "page"
---

<style>
    
    table {
        width: 100%;
    }
    td {
    	
        padding-right:5px;
        padding-left:5px;
    }
    tr:nth-child(even) {background: #EEE}
</style>

**Tutorial at VISAPP 2021 - 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications**

**February 09 2020**

[[Slides](#slides)] [[Further Reading](#reading)]

## Tutorial Description
### Abstract
In recent years, the market has witnessed the appearance of several wearable devices equipped with sensing, processing, and display abilities such as Microsoft HoloLens2, Vuzix Blade, Google Glass and Magic Leap One. Due to their intrinsic mobility and the ability to mix the real and digital worlds through Augmented Reality, such devices are perfect platforms to develop virtual personal assistants capable of seeing the world from the user’s perspective and augment their abilities. In the considered context, sensing goes beyond the collection and analysis of RGB images, with modalities such as depth, IMU, and gaze usually available. Nevertheless, Computer Vision plays a fundamental role in the egocentric perception pipelines of such systems. 

Unlike standard “third person vision”, according to which the processed images and video are acquired from a static point of view neutral to the events, first person (egocentric) vision assumes that images and video are acquired from the non-static and rather “personal” point of view of the user by means of a wearable device. These unique properties make first person (egocentric) vision different from standard third person vision. Most notably, the visual information collected using wearable cameras always provide useful information about the users, revealing what they do, what they pay attention to and how they interact with the world. 

In this tutorial, we will discuss the challenges and opportunities offered by first person (egocentric) vision. We will cover the historical background and seminal works in the field, present the main technological tools (including devices and algorithms) which can be used to analyze first person visual data and discuss challenges and open problems. The last part of the tutorial will specifically focus on an emergent trend of works which focus on the prediction of future events from first person vision, which is paramount for the development of virtual personal assistants.

### Keywords
wearable, first person vision, egocentric vision, augmented reality, visual localization, action recognition, action anticipation

### Aims and learning objectives
The participants will understand the main advantages of first person (egocentric) vision over third person vision to analyze the user’s behavior, build personalized applications and predict future events. Specifically, the participants will learn about: 1) the main differences between third person and first person (egocentric) vision, including the way in which the data is collected and processed, 2) the devices which can be used to collect data and provide services to the users, 3) the algorithms which can be used to manage first person visual data for instance to perform localization, indexing, object detection, action recognition, and the prediction of future events.

### Target audience
First year PhD students, graduate students, researchers.

### Prerequisite Knowledge of Audience
Fundamentals of Computer Vision and Machine Learning (including Deep Learning)

<div id="slides"></div>

## Slides
[Part 1 - Definitions, motivations, history and research trends](http://antoninofurnari.it/downloads/talks/furnari_visapp2021_tutorial_part1.pdf)

[Part 2 - Building Blocks for First Person Vision Systems](http://antoninofurnari.it/downloads/talks/furnari_visapp2021_tutorial_part2.pdf)

<div id="reading"></div>

## Further Reading - References
[First Person Vision @ IPLAB](http://iplab.dmi.unict.it/fpv/)