---
title: 'First Person (Egocentric) Vision: From Augmented Perception to Interaction and Anticipation'
type: "page"
date: 2020-02-09
draft: false
when: "9 Feb. 2020"
venue: "VISAPP 2020 Tutorials, Valletta, MT"
venue_url: "https://visapp.scitevents.org/Tutorials.aspx?y=2020"
slides_url: "/talks/visapp2020"
---

<style>
    
    table {
        width: 100%;
    }
    td {
    	
        padding-right:5px;
        padding-left:5px;
    }
    tr:nth-child(even) {background: #EEE}
</style>

**Tutorial at VISAPP 2020 - 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications**

**February 29 2020**

[[Slides](#slides)] [[Further Reading](#reading)]

## Tutorial Description
### Abstract
Wearable devices equipped with sensing, processing, and display abilities such as Microsoft HoloLens, Google Glass and Magic Leap One allow to perceive the world from user’s point of view. Due to their intrinsic portability and the ability to mix the real and digital worlds, such devices constitute the third wave of computing, after personal computers and smartphones, in which the user plays a central role. Therefore, these wearable devices are ideal candidates for implementing personal intelligent assistants which can understand our behavior and augment our abilities. While in the considered context sensing can go beyond the collection of RGB images and include dedicated depth sensors and IMUs, Computer Vision plays a fundamental role in the egocentric perception pipelines of such systems. 

Unlike standard “third person vision”, which assumes that the processed images and video are acquired from a static point of view neutral to the events, first person (egocentric) vision assumes images and video to be acquired from the non-static and rather “personal” point of view of the user by means of a wearable device. These unique properties make first person (egocentric) vision different from standard third person vision. Most notably, the visual information collected using wearable cameras always “tells something” about the user, revealing what they do, what they pay attention to and how they interact with the world. 

In this tutorial, we will discuss the challenges and opportunities behind first person (egocentric) vision. We will cover the historical background and seminal works, present the main technological tools (including devices and algorithms) which can be used to analyze first person visual data and discuss challenges and open problems.


### Keywords
wearable, first person, egocentric, localization, action recognition, action anticipation

### Aims and learning objectives
The participants will understand the main advantages of first person (egocentric) vision over third person vision to analyze the user’s behavior and build personalized applications. Specifically, the participants will learn about: 1) the main differences between third person and first person (egocentric) vision, including the way in which the data is collected and processed, 2) the devices which can be used to collect data and provide services to the users, 3) the algorithms which can be used to manage first person visual data for instance to perform localization, indexing, action and activity recognition.

### Target audience
First year PhD students, graduate students, researchers.

### Prerequisite Knowledge of Audience
Fundamentals of Computer Vision and Machine Learning (including Deep Learning)

<div id="slides"></div>

## Slides
[Part 1 - Definitions, motivations, history and research trends](http://antoninofurnari.it/downloads/talks/furnari_visapp2020_tutorial_part1.pdf)

[Part 2 - Building Blocks for First Person Vision Systems](http://antoninofurnari.it/downloads/talks/furnari_visapp2020_tutorial_part2.pdf)

<div id="reading"></div>

## Further Reading - References
[First Person Vision @ IPLAB](http://iplab.dmi.unict.it/fpv/)