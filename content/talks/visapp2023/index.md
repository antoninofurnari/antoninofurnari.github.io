---
title: 'A Tutorial on First Person (Egocentric) Vision'
type: "page"
date: 2023-02-19
draft: false
when: "19 Feb. 2023"
venue: "VISAPP 2023 Tutorials, Lisbon, PT"
venue_url: "https://visapp.scitevents.org/Tutorials.aspx?y=2023"
slides_url: "/talks/visapp2023"
---

<style>
    
    table {
        width: 100%;
    }
    td {
    	
        padding-right:5px;
        padding-left:5px;
    }
    tr:nth-child(even) {background: #EEE}
</style>

**Tutorial at VISAPP 2023 - 18th International Conference on Computer Vision Theory and Applications**

**Feb 19 2023**

[[Program & Slides](#program)][[Further Reading](#reading)]

## Tutorial Description
### Abstract
Wearable devices equipped with a camera and computing abilities are attracting the attention of both the market and the society, with commercial devices more and more available and many companies announcing the upcoming release of new devices. The main appeal of wearable devices is due to their mobility and to their ability to enable user-machine interaction through Augmented Reality. Due to these characteristics, wearable devices provide an ideal platform to develop intelligent assistants able to assist humans and augment their abilities, for which Artificial Intelligence and Computer Vision play a major role.

Differently from classic computer vision (the so called “third person vision”), which analyses images collected from a static point of view, first person (egocentric) vision assume that images are collected from the point of view of the user, which gives privileged information on the user’s activities and the way they perceive and interact with the world. Indeed, the visual data acquired with wearable cameras usually provides useful information about the users, their intentions, and how they interact with the world.

This tutorial will discuss the challenges and opportunities offered by first person (egocentric) vision, covering the historical background and seminal works, presenting the main technological tools and building blocks, and discussing applications.


### Keywords
wearable, first person vision, egocentric vision, visual localization, action recognition, action anticipation, object interaction detection

### Aims and learning objectives
The participants will understand the main advantages of first person (egocentric) vision over third person vision to analyze the user’s behavior, build personalized applications and predict future events. Specifically, the participants will learn about: 1) the main differences between third person and first person (egocentric) vision, including the way in which the data is collected and processed, 2) the devices which can be used to collect data and provide services to the users, 3) the algorithms which can be used to manage first person visual data for instance to perform localization, indexing, object detection, action recognition, and the prediction of future events.

<div id="program"></div>

## Program

### [14.45 - 16.30] Part I: Definitions, motivations, history and research trends - Antonino Furnari [<a href="http://antoninofurnari.it/downloads/talks/fpv_tutorial_visapp_2023_part1.pdf">Slides</a>]
 * Agenda of the tutorial; 
 * Definitions, motivations, history and research trends of First Person (egocentric) Vision; 
 * Seminal works in First Person (Egocentric) Vision; 
 * Differences between Third Person and First Person Vision; 
 * First Person Vision datasets; 
 * Wearable devices to acquire/process first person visual data; 
 * Main research trends in First Person (Egocentric) Vision; 


### [16.30 – 17.30] Coffe Break 
### [17.30 - 18.45] Part II: Building Blocks for First Person Vision Systems – Francesco Ragusa [<a href="http://antoninofurnari.it/downloads/talks/fpv_tutorial_visapp_2023_part2.pdf">Slides</a>]
 * Localization;
 * Hand/Object Detection;
 * Action Recognition;
 * Egocentric Human-Object Interaction;
 * Anticipation.
 * Example Applications;
 * Conclusion.


<div id="reading"></div>

## References - Further Reading
 * [First Person Vision @ IPLAB](https://iplab.dmi.unict.it/fpv/)
 * [Next Vision](https://www.nextvisionlab.it)