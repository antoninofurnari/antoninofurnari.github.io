---
title: "Ego-EXTRA: Video-Language Egocentric Dataset for Expert-Trainee Assistance"
date: 2026-02-13T10:00:00+01:00
bibtex: ["@article{Ragusa_2025_Ego-EXTRA,
    author = {Francesco Ragusa and Michele Mazzamuto and Rosario Forte and Irene D'Ambra and James Fort and Jakob Engel and Antonino Furnari and Giovanni Maria Farinella},
    title = {Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance},
    journal = {arXiv preprint arXiv:2512.13238},
    year = {2025},
    pdf = {https://arxiv.org/pdf/2512.13238},
    url = {https://fpv-iplab.github.io/Ego-EXTRA/}
}"]
teaser: "egoextra.png"
video_teaser: false
---

Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user
